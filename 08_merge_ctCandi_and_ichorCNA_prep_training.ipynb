{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candi read files: 427\n",
      "Number of classification files: 427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [00:50<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candi read files: 427\n",
      "Number of classification files: 427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 125/427 [00:16<00:39,  7.63it/s]\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m fulldf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(all_candi_read_files))):\n\u001b[0;32m---> 47\u001b[0m     tmp_readdf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_read_classification_files\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     tmp_candidf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(all_candi_read_files[i], index_col \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     49\u001b[0m     sampleid \u001b[38;5;241m=\u001b[39m all_candi_read_files[i]\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.candi_reads.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "from dataset_paths import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "output_version = \"20241229\"\n",
    "outdir = \"/media/hieunguyen/HNSD_mini/outdir\"\n",
    "\n",
    "mode = \"all\"\n",
    "PROJECT = \"combine_ctcandi_ichorcna\"\n",
    "\n",
    "for dataset_name in [\"VALIDATION\", \"LOD\", \"SPIKE_IN\"]:\n",
    "    metadata = pd.read_csv(f\"./metadata/{dataset_name}.csv\")\n",
    "\n",
    "    for input_cancer_class in [\"CRC\", \"Liver\", \"Lung\", \"Breast\", \"pan_cancer\"]:\n",
    "        path_to_main_output = os.path.join(outdir, PROJECT, output_version, dataset_name)\n",
    "        path_to_07_output = os.path.join(path_to_main_output, f\"07_output_{mode}\", input_cancer_class)\n",
    "        path_to_08_output = os.path.join(path_to_main_output, f\"08_output_{mode}\", input_cancer_class)\n",
    "        os.system(f\"mkdir -p {path_to_08_output}\")\n",
    "        all_read_classification_files = [item for item in pathlib.Path(path_to_07_output).glob(\"*.raw.read_classification.csv\")]\n",
    "        all_candi_read_files = [item for item in pathlib.Path(path_to_07_output).glob(\"*candi_reads.csv\")]\n",
    "\n",
    "        if dataset_name == \"LOD\":\n",
    "            convert_ratio = {\n",
    "            '50' : 0.5, \n",
    "            '100': 1, \n",
    "            '0.5': 0.005, \n",
    "            '25': 0.25, \n",
    "            '15': 0.15, \n",
    "            '5': 0.05, \n",
    "            '1': 0.01, \n",
    "            'HC': 0\n",
    "            }\n",
    "            metadata = metadata[metadata[\"Sample\"].duplicated() == False]\n",
    "            metadata[\"SampleID\"] = metadata[\"Sample\"].values\n",
    "            metadata.columns = [\"ichorCNA\" if item == \"Actual tumor_fraction_ichorCNA\" else item for item in metadata.columns]\n",
    "            metadata[\"spike_in_ratio\"] = metadata[\"spike-in\"].apply(lambda x: convert_ratio[x])\n",
    "            metadata['Label'] = metadata[\"LABEL\"].apply(lambda x: \"CRC\" if x == \"Colorectal cancer\" else x.split(\" \")[0])\n",
    "            metadata[\"spike_in_label\"] = metadata[\"Simulated TF\"].apply(lambda x: \"Control\" if x == \"Healthy-control\" else input_cancer_class)\n",
    "            metadata[\"Label\"] = metadata[[\"Label\", \"spike_in_label\"]].apply(lambda x: x[0] if x[1] != \"Control\" else \"Control\", axis = 1)\n",
    "            metadata = metadata[[\"SampleID\", \"ichorCNA\", \"spike_in_ratio\", \"Label\"]]\n",
    "        elif dataset_name == \"VALIDATION\":\n",
    "            metadata = metadata[[\"SampleID\", \"ichorCNA\"]]\n",
    "            all_candi_read_files = [item for item in all_candi_read_files if item.name.replace(\".candi_reads.csv\", \"\") in metadata[\"SampleID\"].unique()]\n",
    "            all_read_classification_files = [item for item in all_read_classification_files if item.name.replace(\".raw.read_classification.csv\", \"\") in metadata[\"SampleID\"].unique()]\n",
    "        elif dataset_name == \"SPIKE_IN\":\n",
    "            metadata = metadata[[\"SampleID\", \"Spike_in_label\", \"Spike_in_ratio\", \"ichorCNA\"]]\n",
    "            metadata[\"Spike_in_ratio\"] = metadata[\"Spike_in_ratio\"].apply(lambda x: x/100)\n",
    "            metadata.columns = [\"SampleID\", \"Label\", \"spike_in_ratio\", \"ichorCNA\"]\n",
    "            metadata = metadata[[\"SampleID\", \"ichorCNA\", \"spike_in_ratio\", \"Label\"]]\n",
    "        elif dataset_name == \"REPORT4\":\n",
    "            metadata = metadata[[\"SampleID\", \"ichorCNA\", \"Label\"]]\n",
    "            metadata = metadata[metadata[\"ichorCNA\"].isna() == False]\n",
    "\n",
    "        print(f\"Number of candi read files: {len(all_candi_read_files)}\")\n",
    "        print(f\"Number of classification files: {len(all_read_classification_files)}\")\n",
    "\n",
    "        if os.path.isfile(os.path.join(path_to_08_output, \"feature.csv\")) == False:\n",
    "            fulldf = pd.DataFrame()\n",
    "            for i in tqdm(range(len(all_candi_read_files))):\n",
    "                tmp_readdf = pd.read_csv(all_read_classification_files[i], index_col = [0])\n",
    "                tmp_candidf = pd.read_csv(all_candi_read_files[i], index_col = [0])\n",
    "                sampleid = all_candi_read_files[i].name.replace(\".candi_reads.csv\", \"\")\n",
    "                raw_count = tmp_readdf.shape[0]\n",
    "                in_read_count = tmp_readdf[tmp_readdf[\"read_overlap_rate\"] == \"in\"].shape[0]\n",
    "                mean_candi_reads = tmp_candidf.candi_reads.mean()\n",
    "                ratio_raw = mean_candi_reads/raw_count\n",
    "                ratio_in_read = mean_candi_reads/in_read_count\n",
    "                tmpdf = pd.DataFrame({\"SampleID\": sampleid, \n",
    "                                    \"raw_count\": raw_count, \n",
    "                                    \"in_read_count\": in_read_count, \n",
    "                                    \"mean_candi_reads\": mean_candi_reads,\n",
    "                                    \"ratio_raw\": ratio_raw,\n",
    "                                    \"ratio_in_read\": ratio_in_read}, index = [0])\n",
    "\n",
    "                fulldf = pd.concat([fulldf, tmpdf], axis = 0)\n",
    "            fulldf = fulldf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "            fulldf.to_csv(os.path.join(path_to_08_output, \"feature.csv\"))\n",
    "        else:\n",
    "            fulldf = pd.read_csv(os.path.join(path_to_08_output, \"feature.csv\"), index_col = [0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Breast cancer', 'Colorectal cancer', 'Lung cancer',\n",
       "       'Cervical cancer', 'Gastric cancer', 'Ovarian cancer',\n",
       "       'Pancreatic cancer', 'Control', 'Endometric cancer',\n",
       "       'benign breast', 'no lesions', 'multiple benign', 'benign gastric',\n",
       "       'pre-K', 'other benign lesions', 'benign lung', 'Liver cancer',\n",
       "       'lymphoma cancer', 'Prostate cancer'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
